---
layout: post
category: example2
---
<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

2\
暑假在Coursera上学了吴恩达的[机器学习](https://www.coursera.org/learn/machine-learning/)。
仔细介绍了一下监督学习，然后大概介绍了各种无监督学习。
其中最精华的部分在于神经网络的BackPropagation，其他内容都不是很难。

## Back Propagation

本来以为会很麻烦，但实际上也可以用几行代码搞定，多亏了matlab的矩阵运算。
BP算法的本质是偏导数的链式法则，在计算前一层的$\theta_i$的偏导数时候，
只需要考虑当前层的$z_i$的偏导数是多少以及每条边的权值，后面无论有多少层都和前一层无关。
下面的代码同时计算了神经网络学习时候的损失函数和偏导数矩阵：

{% matlab linenos %}
function [J grad] = nnCostFunction(nn_params, ...
                                   input_layer_size, ...
                                   hidden_layer_size, ...
                                   num_labels, ...
                                   X, y, lambda)
% NNCOSTFUNCTION Implements the neural network cost function
% for a two layer neural network which performs classification

% Reshape nn_params back into the parameters Theta1 and Theta2, 
% the weight matrices for our 2 layer neural network

Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                 hidden_layer_size, (input_layer_size + 1));

Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                 num_labels, (hidden_layer_size + 1));

% Setup some useful variables
m = size(X, 1);
     
% You need to return the following variables correctly 
J = 0;
Theta1_grad = zeros(size(Theta1));
Theta2_grad = zeros(size(Theta2));

% ====================== YOUR CODE HERE ======================
% Instructions: You should complete the code by working through
%               the following parts.
%
% Part 1: Feedforward the neural network and return the cost in the
%   variable J. After implementing Part 1, you can verify that your
%   cost function computation is correct by verifying the cost
%   computed in ex4.m
%
% Part 2: Implement the backpropagation algorithm to compute the 
%  gradients Theta1_grad and Theta2_grad. You should return the
%  partial derivatives of the cost function with respect to
%  Theta1 and Theta2 in Theta1_grad and Theta2_grad, respectively.
%  After implementing Part 2, you can check that your implementation 
%  is correct by running checkNNGradients.
%
%  Note: The vector y passed into the function is a vector of
%        labels containing values from 1..K. You need to map this
%        vector into a binary vector of 1's and 0's to be used with
%        the neural network cost function.
%
%  Hint: We recommend implementing backpropagation using a for-loop
%        over the training examples if you are implementing it for
%        the first time.
%
% Part 3: Implement regularization with the cost function and
%        gradients.
%
%  Hint: You can implement this around the code for
%        backpropagation. That is, you can compute the gradients
%        for the regularization separately and then add them to
%        Theta1_grad and Theta2_grad from Part 2.
%

% Forward Propagation
a1=[ones(size(X,1),1) X];
z2=a1*Theta1';
a2=[ones(size(z2,1),1) sigmoid(z2)];
z3=a2*Theta2';
a3=sigmoid(z3);
a=[1:1:num_labels];
Y=(y==a);

J=sum(-sum(Y.*log(a3))-sum((1-Y).*log(1-a3)))/m + ...
    lambda*(sum(Theta1.*Theta1)*[0 ones(1,size(Theta1,2)-1)]' + ...
    sum(Theta2.*Theta2)*[0 ones(1,size(Theta2,2)-1)]')/(2*m);

% Back Propagation
pz3=a3-Y;
pz2=pz3*Theta2;
pz2=pz2(:,2:end).*sigmoidGradient(z2);
Theta1_grad=pz2'*a1/m;
Theta2_grad=pz3'*a2/m;

%Regularization

Theta1_grad=Theta1_grad + ...
    lambda*[zeros(size(Theta1,1),1) Theta1(:,2:end)]/m;
Theta2_grad=Theta2_grad + ...
    lambda*[zeros(size(Theta2,1),1) Theta2(:,2:end)]/m;

% ------------------------------------------------------

% =================================================================
% Unroll gradients
grad = [Theta1_grad(:) ; Theta2_grad(:)];
end
```
{% endmatlab %}

可以看到BackPropagation的部分其实并不复杂。
每个$pz_i$所代表的就是$\frac{\partial J}{\partial z_i}$，也即损失函数J对$z_i$的偏导数。
这里最重要的是要区分清楚$a_i$和$z_i$的关系，即$a_i=sigmoid(z_i)$。
深入研究一下会发现必须要用$z_i$的偏导数才能把代码写的那么简洁。

这门课程所有编程习题的答案都在[这里](https://github.com/hyf20010101/machine_learning)。
代码风格比较简洁，用吴恩达的话说是比较vectorized，如果想要更易懂的版本可以搜搜别人发到Github上的内容。
